{"name":"Music and Lyrics","tagline":"Genre classification based on musical features and lyrics.","body":"### Abstract\r\nOur goal is to make a system that is able to determine a song’s genre, based on a combination of low-level audio features and lyrical content. We tend to combine the most relevant low level and lyrical features, utilizing boosting to create the most powerful classification tool we can.\r\n\r\n### Motivation\r\nOur interest in this topic arises from a passion for music, and the process of classifying genres with a machine learning method will reveal a lot about the fundamental characteristics of different genres and the underlying mathematical nature of all music.\r\n\r\nThis work has many practical applications.  For instance, it may be used for cataloguing music when it’s uploaded to a library, or as a basis for a song suggestion application. In theory, a major extension of this learning algorithm could even allow for computer construction of basic song forms of a particular genre.\r\n\r\nThe applications of this feature extraction and use may also reveal important characteristics about what lyrical tendencies and audio features are determinant in a song's qualities. This knowledge has applications beyond genre classification and could be used for many other applications of audio analysis and classification.\r\n\r\n### Tools\r\nWe are using Marsyas (Music Analysis, Retrieval and Synthesis for Audio Signals), an open-source audio processing framework written by George Tzanetakis. This framework provides a number of useful tools for Music Information Retrieval applications like ours, and we use this software to extract features from audio files in our dataset.\r\n\r\nWe are also using the ChartLyrics API in order to fetch lyrics for each of the songs. These lyrics are used alongside audio features as input to our learner.\r\n\r\n### Audio Features\r\nIn our initial analysis of data, we have begun work with two main audio feature classifiers. We have a matlab script that compiles tempo information on all of our songs, for use classification in WEKA. Tempo is one of our initial features because of its dramatic variation between different styles of music, a distinction that may prove useful in distinguishing between faster (metal) and slower (reggae) genres. Our additional audio feature is Mel-Frequency Cepstral Coefficient (MFCC) analysis. MFCCs are designed to capture short-term spectral-based features and, though initially developed for vocal recognition, have proven highly effective in musical classification. \r\nResearch by Tzanetakis suggests that pitch is also a power classification tool for genre classification. Unfortunately, we've had a number of issues attempting to extract pitch. Despite attempts with python binding enabled Marsyas, and older versions of Marsyas, we have yet to extract meaningful input. This is something we hope to achieve in the week. Alternatively, we will research other meaningful audio features and find another to extract.\r\n\r\n### Lyrical Analysis & Classification\r\nWe collected lyrics data using a python script in conjunction with the ChartLyrics API. We created a list of song and artist names contained in the GTZAN dataset based on a list provided by George T., then pulled the lyrics for these tracks from ChartLyrics. We decided not to attempt lyrics-based classification for jazz and classical because songs from these two genres are mostly instrumentals.\r\n\r\nEven though we were unable to find the lyrics for some songs in the database, and some songs in the dataset turned out to be instrumentals, we were able to successfully scrape lyrics for >90% of the songs that we tried. Using a two-dimensional classifier (specifically, training/classifying country vs. disco) yielded a prediction accuracy of ~75%. Even though we are yet to run formal statistical analyses, it seems evident that this is better than the \"blind\" prediction accuracy of 50%. \r\n\r\n### Initial Results: Audio Features\r\nWe've had success in using Marsyas to extract features from the audio files in our dataset, of which there are 100 in each of 8 genres. We first created a collection file for each genre:\r\n```\r\n$ ./mkcollection -c blues.mf -l blues ~/Desktop/genres/blues/\r\n$ ./mkcollection -c country.mf -l country ~/Desktop/genres/country/ \r\n$ ./mkcollection -c disco.mf -l disco ~/Desktop/genres/disco/\r\n$ ./mkcollection -c hiphop.mf -l hiphop ~/Desktop/genres/hiphop/\r\n$ ./mkcollection -c metal.mf -l metal ~/Desktop/genres/metal/\r\n$ ./mkcollection -c pop.mf -l pop ~/Desktop/genres/pop/\r\n$ ./mkcollection -c reggae.mf -l reggae ~/Desktop/genres/reggae/\r\n$ ./mkcollection -c rock.mf -l rock ~/Desktop/genres/rock/\r\n```\r\n\r\nand then used these collections to extract timbral features (including MFCC's) based on genre.\r\n\r\n```\r\n$ ./bextract -sv -timbral --TimbralFeatures blues.mf country.mf disco.mf hiphop.mf metal.mf pop.mf reggae.mf rock.mf -w genres.arff \r\n```\r\n\r\nThe \"genres.arff\" file generated by this command is a data file in a format that can be immediately imported into Weka for analysis. The file includes timbral features for all 800 audio files, sorted evenly into the 8 genres under consideration. After importing this data into Weka, we can easily run 10-fold cross validation with a variety of machine learning methods. With cross validation in this case, for each fold we train on 720 songs and validate with the other 80.\r\n\r\nA simple Bayes Net classifier achieved an average 54.625% accuracy in classifying genres over 10 folds. This is not great accuracy, but is clearly better than the prior probability of 12.5% (1/8) that we'd get by guessing randomly as to what genre a song belongs to. A confusion matrix for this classification is shown below:\r\n\r\n```\r\n=== Confusion Matrix ===\r\n\r\n  a  b  c  d  e  f  g  h   <-- classified as\r\n 52 13  4  2  4  9  4 12 |  a = blues\r\n 12 64  6  0  5  0  3 10 |  b = country\r\n  2  6 52  4  5  7 13 11 |  c = disco\r\n  1  0 14 36  2 23 24  0 |  d = hiphop\r\n  0  3  1  0 74  3  1 18 |  e = metal\r\n  6  6  9  9  1 63  3  3 |  f = pop\r\n  4  8 13  8  0  6 56  5 |  g = reggae\r\n  7 17 11  0 17  1  7 40 |  h = rock\r\n```\r\n\r\n\r\n### Initial Results: Lyrics\r\n\r\nWe're still scraping lyrics from ChartLyrics.  We currently have 198 files in five genres. With five genres (Blues, Country, Disco, Hip Hop, and Metal), we pulled out five files from each for testing and used the rest to train the learner.  This means we trained with 24 blues files, 40 country files, 33 disco files, 49 hip hop files, and 27 metal files, or 173 files total.\r\n\r\nWe found that we correctly classified 64% of songs.  Below is the confusion matrix.  It looks like we mainly misclassified songs as hip hop, which might simply be due to the fact that there are SO MANY words in hip hop songs.  We classified 12 songs as hip hop even though only 5 were hip hop.  We may need to introduce an additional heuristic that just counts the number of words in a song.  Most hip hop songs are probably longer than most blues songs.\r\n\r\nThe confusion matrix for these initial results is shown below:\r\n\r\n![Confusion matrix for lyrics results](https://dl.dropboxusercontent.com/u/61350515/lyrics.png)\r\n\r\n### Dataset\r\nWe’re using the GTZAN Dataset, which contains 10 genres with 100 musical excerpts each. Given our focus on lyrics, we've decided to exclude jazz and classical since they are mainly instrumental genres. The audio files are each 30 seconds long, and are mono .wav files with a sample rate of 22,050 Hz and a bit depth of 16 bits per sample.\r\n\r\nThe audio comes from a number of sources, including radio and field recordings in addition to excerpts from normal studio-recorded CD’s. This variation allows for a higher level of robustness in evaluating our learner.\r\n\r\nAs provided directly from the Marsyas website, the GTZAN audio excerpts are unlabeled -- each filename is just the genre and a number from 0 to 99. However, Bob Sturm has provided song names and artists for most of the excerpts (his paper with this information is cited below). We have filled in in some of the gaps in his labeling using song identification tools like Shazam, and we use this information in order to fetch lyrics for the songs.\r\n\r\n\r\n### Our Team\r\nMoritz Gellner (@moritzg91)\r\n\r\nJosh Jacobson (@josh-jacobson)\r\n\r\nCarson Potter (@cpottamus)\r\n\r\nSam Toizer (@sammysf)\r\n\r\nWe're all seniors at Northwestern University, and this is our final project for Machine Learning (EECS 349) with Bryan Pardo.\r\n\r\n### References\r\nBasili et al. “Classification of Musical Genre: A Machine Learning Approach.”\r\n\r\nCuthbert, et al. \"Feature Extraction and Machine Learning on Symbolic Music Using the music21 Toolkit\". 12th International Society for Music Information Retrieval Conference, 2011. <http://ismir2011.ismir.net/papers/PS3-6.pdf>.\r\n\r\nLi, et al. \"A Comparative Study on Content-Based Music Genre Classification\". SIGIR. 2003, July 28 - August 1. Web. 1 Dec., 2013.\r\n\r\nLidy, Thomas. \"Marsyas and Rhythm Patterns: Evaluation of two music genre classification systems.\" <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.1952&rep=rep1&type=pdf>.\r\n\r\nMandel, et al. \"Song Level Features and Support Vector Machines for Music Classification\". 2005, Queen Mary University, London.\r\n\r\nMathieu, et al. \"Yaafe, an Easy to Use and Efficient Audio Feature Extraction Software.\" Institut Telecom. <http://biblio.telecom-paristech.fr/cgi-bin/download.cgi?id=10395>.\r\n\r\nSturm, Bob L., 2012. \"An Analysis of the GTZAN Music Genre Dataset\", Proc. ACM Workshop MIRUM, Nara, Japan, Nov. 2012. <http://doi.acm.org/10.1145/2390848.2390851>.\r\n\r\nTzanetakis, George. \"Marsyas: A Framework For Audio Analysis.\" Web. 2 Dec. 2013. <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.4572&rep=rep1&type=pdf>\r\n\r\nTzanetakis, George. \"Marsyas-0.2: A Case Study in Implementing Music Information Retrieval Systems.\" Web. 2 Dec. 2013.\r\n\r\nTzanetakis, et al. \"Pitch Histograms in Audio and Symbolic Music Information Retrieval.\" <http://www.cs.cmu.edu/~gtzan/work/pubs/ismir02gtzan.pdf>.\r\n\r\nTzanetakis, G., and Cook, P. \"Musical Genre Classification of Audio Signals.\" IEEE Transactions on Speech and Audio Processing 10.5 (2002): 293-302. Web. 12 Nov. 2013. <http://dspace.library.uvic.ca:8080/bitstream/handle/1828/1344/tsap02gtzan.pdf?sequence=1>.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}