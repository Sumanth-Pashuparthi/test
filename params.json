{"name":"Music and Lyrics","tagline":"Genre classification based on musical features and lyrics.","body":"### Abstract\r\nOur goal is to make a system that is able to determine a song’s genre, based on a combination of low-level audio features and lyrical content. We tend to combine the most relevant low level and lyrical features, utilizing boosting to create the most powerful classification tool we can.\r\n\r\n### Motivation\r\nOur interest in this topic arises from a passion for music, and the process of classifying genres with a machine learning method will reveal a lot about the fundamental characteristics of different genres and the underlying mathematical nature of all music.\r\n\r\nThis work has many practical applications.  For instance, it may be used for cataloguing music when it’s uploaded to a library, or as a basis for a song suggestion application. In theory, a major extension of this learning algorithm could even allow for computer construction of basic song forms of a particular genre.\r\n\r\nThe applications of this feature extraction and use may also reveal important characteristics about what lyrical tendencies and audio features are determinant in a song's qualities. This knowledge has applications beyond genre classification and could be used for many other applications of audio analysis and classification.\r\n\r\n### Tools\r\nWe are using Marsyas (Music Analysis, Retrieval and Synthesis for Audio Signals), an open-source audio processing framework written by George Tzanetakis. This framework provides a number of useful tools for Music Information Retrieval applications like ours, and we use this software to extract features from audio files in our dataset.\r\n\r\nWe are also using the ChartLyrics API in order to fetch lyrics for each of the songs. These lyrics are used alongside audio features as input to our learner.\r\n\r\n### Initial Results\r\nWe've had success in using Marsyas to extract features from the audio files in our dataset, of which there are 100 in each of 8 genres. We first created a collection file for each genre:\r\n```\r\n$ ./mkcollection -c blues.mf -l blues ~/Desktop/genres/blues/\r\n$ ./mkcollection -c country.mf -l country ~/Desktop/genres/country/ \r\n$ ./mkcollection -c disco.mf -l disco ~/Desktop/genres/disco/\r\n$ ./mkcollection -c hiphop.mf -l hiphop ~/Desktop/genres/hiphop/\r\n$ ./mkcollection -c metal.mf -l metal ~/Desktop/genres/metal/\r\n$ ./mkcollection -c pop.mf -l pop ~/Desktop/genres/pop/\r\n$ ./mkcollection -c reggae.mf -l reggae ~/Desktop/genres/reggae/\r\n$ ./mkcollection -c rock.mf -l rock ~/Desktop/genres/rock/\r\n```\r\n\r\nand then used these collections to extract features based on genre.\r\n\r\n```\r\n$ ./bextract -sv blues.mf country.mf disco.mf -w genres.arff -p genres.mpl \r\n```\r\n\r\nOur results.\r\n\r\n### Dataset\r\nWe’re considering using the GTZAN Dataset, which contains 1,000 musical excerpts, each 30 seconds long. The dataset represents an even distribution of 10 genres, so there are 100 songs of each genre. The audio files are all mono and in .wav format, with a sample rate of 22,050 Hz and a bit depth of 16 bits per sample.\r\n\r\nThe audio comes from a number of sources, including radio and field recordings in addition to excerpts from normal studio-recorded CD’s. This variation allows for a higher level of robustness in evaluating our learner. The large volume of recordings in the dataset will also allow us flexibility in constructing training and validation sets, and potentially evaluating the learning algorithm using cross-validation.\r\n\r\nThe biggest probably issue with the GTZAN dataset is the incomplete labelling of artists and titles, due to the large period of collection of the songs by Tzanetakis. We're working with Professor Pardo to either implement a solution using a song discovery tool such as Shazam or SoundHound, or to find an alternative, more labelled, dataset. Knowing the song information is critical for finding the lyrics.\r\n\r\n\r\n### Our Team\r\nMoritz Gellner (@moritzg91)\r\n\r\nJosh Jacobson (@josh-jacobson)\r\n\r\nCarson Potter (@cpottamus)\r\n\r\nSam Toizer (@sammysf)\r\n\r\nWe're all seniors at Northwestern University, and this is our final project for Machine Learning (EECS 349) with Bryan Pardo.\r\n\r\n### References\r\nBasili et al. “Classification of Musical Genre: A Machine Learning Approach.”\r\n\r\nCuthbert, et al. \"Feature Extraction and Machine Learning on Symbolic Music Using the music21 Toolkit\". 12th International Society for Music Information Retrieval Conference, 2011. <http://ismir2011.ismir.net/papers/PS3-6.pdf>.\r\n\r\nLidy, Thomas. \"Marsyas and Rhythm Patterns: Evaluation of two music genre classification systems.\" <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.1952&rep=rep1&type=pdf>.\r\n\r\n\r\nMathieu, et al. \"Yaafe, an Easy to Use and Efficient Audio Feature Extraction Software.\" Institut Telecom. <http://biblio.telecom-paristech.fr/cgi-bin/download.cgi?id=10395>.\r\n\r\nSturm, Bob L., 2012. \"An Analysis of the GTZAN Music Genre Dataset\", Proc. ACM Workshop MIRUM, Nara, Japan, Nov. 2012. <http://doi.acm.org/10.1145/2390848.2390851>.\r\n\r\nTzanetakis, George. \"Marsyas: A Framework For Audio Analysis.\" Web. 2 Dec. 2013. <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.4572&rep=rep1&type=pdf>\r\n\r\nTzanetakis, George. \"Marsyas-0.2: A Case Study in Implementing Music Information Retrieval Systems.\" Web. 2 Dec. 2013. <https://www.google.com/urlsa=t&rct=j&q=&esrc=s&source=web&cd=6&ved=0CF0QFjAF&url=http%3A%2F%2Fwww.researchgate.net%2Fpublication%2F228373711_Marsyas0.2_a_case_study_in_implementing_music_information_retrieval_systems%2Ffile%2F79e4150f71a5d0e62b.pdf&ei=iZieUu_rGdPeyQHWwoDIDA&usg=AFQjCNEUwdIRKlfuVB8RQ0FNCI6su7_YVA&sig2=S1YSDrzXWHqGndiJzVfYTA&bvm=bv.57155469,d.aWc&cad=rja>.\r\n\r\nTzanetakis, et al. \"Pitch Histograms in Audio and Symbolic Music Information Retrieval.\" <http://www.cs.cmu.edu/~gtzan/work/pubs/ismir02gtzan.pdf>.\r\n\r\nTzanetakis, G., and Cook, P. \"Musical Genre Classification of Audio Signals.\" IEEE Transactions on Speech and Audio Processing 10.5 (2002): 293-302. Web. 12 Nov. 2013. <http://dspace.library.uvic.ca:8080/bitstream/handle/1828/1344/tsap02gtzan.pdf?sequence=1>.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}