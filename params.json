{"name":"Music and Lyrics","tagline":"Genre classification based on musical features and lyrics.","body":"### Abstract\r\nOur goal is to make a system that is able to determine a song’s genre, based on a combination of low-level audio features and lyrical content. We tend to combine the most relevant low level and lyrical features, utilizing boosting to create the most powerful classification tool we can.\r\n\r\n### Motivation\r\nOur interest in this topic arises from a passion for music, and the process of classifying genres with a machine learning method will reveal a lot about the fundamental characteristics of different genres and the underlying mathematical nature of all music.\r\n\r\nThis work has many practical applications.  For instance, it may be used for cataloguing music when it’s uploaded to a library, or as a basis for a song suggestion application. In theory, a major extension of this learning algorithm could even allow for computer construction of basic song forms of a particular genre.\r\n\r\nThe applications of this feature extraction and use may also reveal important characteristics about what lyrical tendencies and audio features are determinant in a song's qualities. This knowledge has applications beyond genre classification and could be used for many other applications of audio analysis and classification.\r\n\r\n### Tools\r\nWe are using Marsyas (Music Analysis, Retrieval and Synthesis for Audio Signals), an open-source audio processing framework written by George Tzanetakis. This framework provides a number of useful tools for Music Information Retrieval applications like ours, and we use this software to extract features from audio files in our dataset.\r\n\r\nWe are also using the ChartLyrics API in order to fetch lyrics for each of the songs. These lyrics are used alongside audio features as input to our learner.\r\n\r\n### Initial Results\r\nWe've had success in using Marsyas to extract features from the audio files in our dataset, of which there are 100 in each of 8 genres. We first created a collection file for each genre:\r\n```\r\n$ ./mkcollection -c blues.mf -l blues ~/Desktop/genres/blues/\r\n$ ./mkcollection -c country.mf -l country ~/Desktop/genres/country/ \r\n$ ./mkcollection -c disco.mf -l disco ~/Desktop/genres/disco/\r\n$ ./mkcollection -c hiphop.mf -l hiphop ~/Desktop/genres/hiphop/\r\n$ ./mkcollection -c metal.mf -l metal ~/Desktop/genres/metal/\r\n$ ./mkcollection -c pop.mf -l pop ~/Desktop/genres/pop/\r\n$ ./mkcollection -c reggae.mf -l reggae ~/Desktop/genres/reggae/\r\n$ ./mkcollection -c rock.mf -l rock ~/Desktop/genres/rock/\r\n```\r\n\r\nand then used these collections to extract features based on genre.\r\n\r\n```\r\n$ ./bextract -sv blues.mf country.mf disco.mf -w genres.arff -p genres.mpl \r\n```\r\n\r\nOur results.\r\n\r\n### Dataset\r\nWe’re using the GTZAN Dataset, which contains 10 genres with 100 musical excerpts each. Given our focus on lyrics, we've decided to exclude jazz and classical since they are mainly instrumental genres. The audio files are each 30 seconds long, and are mono .wav files with a sample rate of 22,050 Hz and a bit depth of 16 bits per sample.\r\n\r\nThe audio comes from a number of sources, including radio and field recordings in addition to excerpts from normal studio-recorded CD’s. This variation allows for a higher level of robustness in evaluating our learner.\r\n\r\nAs provided directly from the Marsyas website, the GTZAN audio excerpts are unlabeled -- each filename is just the genre and a number from 0 to 99. However, Bob Sturm has provided song names and artists for most of the excerpts (his paper with this information is cited below). We have filled in in some of the gaps in his labeling using song identification tools like Shazam, and we use this information in order to fetch lyrics for the songs.\r\n\r\n\r\n### Our Team\r\nMoritz Gellner (@moritzg91)\r\n\r\nJosh Jacobson (@josh-jacobson)\r\n\r\nCarson Potter (@cpottamus)\r\n\r\nSam Toizer (@sammysf)\r\n\r\nWe're all seniors at Northwestern University, and this is our final project for Machine Learning (EECS 349) with Bryan Pardo.\r\n\r\n### References\r\nBasili et al. “Classification of Musical Genre: A Machine Learning Approach.”\r\n\r\nCuthbert, et al. \"Feature Extraction and Machine Learning on Symbolic Music Using the music21 Toolkit\". 12th International Society for Music Information Retrieval Conference, 2011. <http://ismir2011.ismir.net/papers/PS3-6.pdf>.\r\n\r\nLidy, Thomas. \"Marsyas and Rhythm Patterns: Evaluation of two music genre classification systems.\" <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.1952&rep=rep1&type=pdf>.\r\n\r\n\r\nMathieu, et al. \"Yaafe, an Easy to Use and Efficient Audio Feature Extraction Software.\" Institut Telecom. <http://biblio.telecom-paristech.fr/cgi-bin/download.cgi?id=10395>.\r\n\r\nSturm, Bob L., 2012. \"An Analysis of the GTZAN Music Genre Dataset\", Proc. ACM Workshop MIRUM, Nara, Japan, Nov. 2012. <http://doi.acm.org/10.1145/2390848.2390851>.\r\n\r\nTzanetakis, George. \"Marsyas: A Framework For Audio Analysis.\" Web. 2 Dec. 2013. <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.83.4572&rep=rep1&type=pdf>\r\n\r\nTzanetakis, George. \"Marsyas-0.2: A Case Study in Implementing Music Information Retrieval Systems.\" Web. 2 Dec. 2013.\r\n\r\nTzanetakis, et al. \"Pitch Histograms in Audio and Symbolic Music Information Retrieval.\" <http://www.cs.cmu.edu/~gtzan/work/pubs/ismir02gtzan.pdf>.\r\n\r\nTzanetakis, G., and Cook, P. \"Musical Genre Classification of Audio Signals.\" IEEE Transactions on Speech and Audio Processing 10.5 (2002): 293-302. Web. 12 Nov. 2013. <http://dspace.library.uvic.ca:8080/bitstream/handle/1828/1344/tsap02gtzan.pdf?sequence=1>.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}